{"paperId":{"0":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","1":"17d5884215b5afa53545cd7cb6135de5478da4ec","2":"7a064df1aeada7e69e5173f7d4c8606f4470365b","3":"748629cb0b8e5a5708e1c6605f71b36eb525a3ce","4":"2ffcf8352223c95ae8cef4daaec995525ecc926b","5":"9f1c5777a193b2c3bb2b25e248a156348e5ba56d","6":"2bc1c8bd00bbf7401afcb5460277840fd8bab029","7":"477d66dcd2c08243dcc69822d6da7ec06393773a","8":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","9":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","10":"1359d2ef45f1550941e22bf046026c89f6edf315","11":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","12":"a54b56af24bb4873ed0163b77df63b92bd018ddc","13":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","14":"4fa37d012ad0014552a6a5a03624b29f95558bf7","15":"0e002114cd379efaca0ec5cda6d262b5fe0be104","16":"75352cc69a29bd5fc411e0e79737cb96b6309161","17":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","18":"063f8b1ecf2394ca776ac61869734de9c1953808","19":"d56c1fc337fb07ec004dc846f80582c327af717c"},"url":{"0":"https:\/\/www.semanticscholar.org\/paper\/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","1":"https:\/\/www.semanticscholar.org\/paper\/17d5884215b5afa53545cd7cb6135de5478da4ec","2":"https:\/\/www.semanticscholar.org\/paper\/7a064df1aeada7e69e5173f7d4c8606f4470365b","3":"https:\/\/www.semanticscholar.org\/paper\/748629cb0b8e5a5708e1c6605f71b36eb525a3ce","4":"https:\/\/www.semanticscholar.org\/paper\/2ffcf8352223c95ae8cef4daaec995525ecc926b","5":"https:\/\/www.semanticscholar.org\/paper\/9f1c5777a193b2c3bb2b25e248a156348e5ba56d","6":"https:\/\/www.semanticscholar.org\/paper\/2bc1c8bd00bbf7401afcb5460277840fd8bab029","7":"https:\/\/www.semanticscholar.org\/paper\/477d66dcd2c08243dcc69822d6da7ec06393773a","8":"https:\/\/www.semanticscholar.org\/paper\/b0b0dddb8310e01b9407a21674c2d33a23a6e967","9":"https:\/\/www.semanticscholar.org\/paper\/80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","10":"https:\/\/www.semanticscholar.org\/paper\/1359d2ef45f1550941e22bf046026c89f6edf315","11":"https:\/\/www.semanticscholar.org\/paper\/f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","12":"https:\/\/www.semanticscholar.org\/paper\/a54b56af24bb4873ed0163b77df63b92bd018ddc","13":"https:\/\/www.semanticscholar.org\/paper\/18318b10e7c2dd4ad292208f4399eb1d4dca5768","14":"https:\/\/www.semanticscholar.org\/paper\/4fa37d012ad0014552a6a5a03624b29f95558bf7","15":"https:\/\/www.semanticscholar.org\/paper\/0e002114cd379efaca0ec5cda6d262b5fe0be104","16":"https:\/\/www.semanticscholar.org\/paper\/75352cc69a29bd5fc411e0e79737cb96b6309161","17":"https:\/\/www.semanticscholar.org\/paper\/c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","18":"https:\/\/www.semanticscholar.org\/paper\/063f8b1ecf2394ca776ac61869734de9c1953808","19":"https:\/\/www.semanticscholar.org\/paper\/d56c1fc337fb07ec004dc846f80582c327af717c"},"title":{"0":"Reducing Transformer Depth on Demand with Structured Dropout","1":"CERT: Contrastive Self-supervised Learning for Language Understanding","2":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","3":"On Layer Normalization in the Transformer Architecture","4":"Adversarial Training for Large Neural Language Models","5":"Cloze-driven Pretraining of Self-attention Networks","6":"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training","7":"Multilingual is not enough: BERT for Finnish","8":"Byte Pair Encoding is Suboptimal for Language Model Pretraining","9":"Patient Knowledge Distillation for BERT Model Compression","10":"AraBERT: Transformer-based Model for Arabic Language Understanding","11":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training","12":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","13":"CLUE: A Chinese Language Understanding Evaluation Benchmark","14":"CamemBERT: a Tasty French Language Model","15":"MPNet: Masked and Permuted Pre-training for Language Understanding","16":"Distilling Knowledge Learned in BERT for Text Generation","17":"Linformer: Self-Attention with Linear Complexity","18":"AdapterHub: A Framework for Adapting Transformers","19":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},"year":{"0":2019,"1":2020,"2":2019,"3":2020,"4":2020,"5":2019,"6":2019,"7":2019,"8":2020,"9":2019,"10":2020,"11":2020,"12":2019,"13":2020,"14":2019,"15":2020,"16":2019,"17":2020,"18":2020,"19":2019},"referenceCount":{"0":63,"1":42,"2":83,"3":53,"4":71,"5":43,"6":46,"7":52,"8":31,"9":41,"10":52,"11":47,"12":23,"13":41,"14":83,"15":31,"16":58,"17":32,"18":56,"19":62},"citationCount":{"0":253,"1":125,"2":2821,"3":224,"4":75,"5":154,"6":402,"7":157,"8":55,"9":381,"10":282,"11":175,"12":2221,"13":112,"14":389,"15":121,"16":63,"17":437,"18":141,"19":143},"fieldsOfStudy":{"0":["Computer Science","Mathematics"],"1":["Computer Science","Mathematics"],"2":["Computer Science"],"3":["Computer Science","Mathematics"],"4":["Computer Science"],"5":["Computer Science"],"6":["Computer Science"],"7":["Computer Science"],"8":["Computer Science"],"9":["Computer Science"],"10":["Computer Science"],"11":["Computer Science"],"12":["Computer Science"],"13":["Computer Science"],"14":["Computer Science"],"15":["Computer Science"],"16":["Computer Science"],"17":["Computer Science","Mathematics"],"18":["Computer Science"],"19":["Computer Science"]}}